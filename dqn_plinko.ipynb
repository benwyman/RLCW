{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the Plinko DQN algorithm using Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "import copy # deep copying Q-table\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Double Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation: \n",
    ">The original Plinko code uses standard Q-learning. Q-learning is known for maximization bias, leading to overestimation of action values. Our standard Q-learning algorithm uses one Q-table to select both the best next action and to evaluate the value of that action. If some action's value is overestimated our max operation will likely select it therefore distributing the overestimation. Double Q-learning ensures that our selection and evaluation are separate. We will use the online Q-table to select the best next action while using the target Q-table to evaluate the value of that chosen action. This will reduce the chance of consistently selecting actions based on overestimated values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation: \n",
    ">We expect more accurate Q-value estimates, which will hopefully result in a more stable learning process and convergence to a better final policy to ensure a higher success rate for the target bucket. It might also prevent our agent from getting stuck favouring sub-optimal paths due to early overestimations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Trackers and Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipes = {}  # maps (x, y) of pipe end -> (x, y) of connected destination\n",
    "blocks = {}  # maps row_y -> {x: original tile} for restoring blocked rows\n",
    "\n",
    "# Q-Learning Specific\n",
    "# Two Q-tables for Double DQN\n",
    "q_table_online = defaultdict(lambda: defaultdict(float))\n",
    "q_table_target = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "# Experience Replay\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "replay_buffer = deque(maxlen=10000) # store last 10k transitions\n",
    "batch_size = 64\n",
    "\n",
    "# Statistics Trackers\n",
    "bucket_tracker = {i: 0 for i in range(5)}  # maps bucket index -> number of landings\n",
    "ledge_tracker = defaultdict(int)  # maps state tuple -> number of visits\n",
    "block_row_tracker = defaultdict(int) # maps block state tuple -> number of visits\n",
    "spike_tracker = defaultdict(int)  # maps spike row y -> number of hits\n",
    "pipe_tracker = defaultdict(int)  # maps (x, y) of pipe entry/exit -> number of uses\n",
    "button_tracker = defaultdict(int)  # maps (x, y) of button tile -> number of presses\n",
    "episode_rewards = [] # track rewards per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Board Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid(width, height):\n",
    "    grid = {}\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            if (y % 2 == 0 and x % 2 == 1) or (y % 2 == 1 and x % 2 == 0):\n",
    "                grid[(x, height - 1 - y)] = 'O'  # place pegs in a checkered pattern\n",
    "            else:\n",
    "                grid[(x, height - 1 - y)] = ' '  # empty spaces between pegs\n",
    "    return grid\n",
    "\n",
    "def mark_ledge(grid, start_x, length, ledge_y, button_x=None):\n",
    "    # place a horizontal ledge starting at start_x on row ledge_y\n",
    "    for x in range(start_x, start_x + length):\n",
    "        if (x, ledge_y) not in grid or grid[(x, ledge_y)] == ' ': # Avoid overwriting pegs/other features unless empty\n",
    "            if x == button_x:\n",
    "                grid[(x, ledge_y)] = '⬒'  # mark a special button tile\n",
    "                button_tracker[(x, ledge_y)]  # initialize button in tracker\n",
    "            else:\n",
    "                grid[(x, ledge_y)] = '_'  # normal ledge tile\n",
    "    # Ledge tracker now tracks state visits, not just ledge definitions\n",
    "\n",
    "def mark_spike(grid, start_x, length, spike_y):\n",
    "    for x in range(start_x, start_x + length):\n",
    "            if (x, spike_y) not in grid or grid[(x, spike_y)] == ' ':\n",
    "                grid[(x, spike_y)] = '^'\n",
    "    spike_tracker[spike_y]  # auto-initializes to 0 if not already set\n",
    "\n",
    "def mark_pipe(grid, x, y1, y2):\n",
    "    # mark a vertical pipe that connects y1 and y2 at column x\n",
    "    top = max(y1, y2)\n",
    "    bottom = min(y1, y2)\n",
    "\n",
    "    for y in range(bottom, top + 1):\n",
    "        if y == top:\n",
    "            grid[(x, y)] = '⤓'  # down pipe entrance\n",
    "        elif y == bottom:\n",
    "            grid[(x, y)] = '↥'  # up pipe entrance\n",
    "        else:\n",
    "            tile = grid.get((x, y), ' ')\n",
    "            grid[(x, y)] = 'Φ' if tile == 'O' else '|'  # middle of the pipe\n",
    "\n",
    "    # connect both ends in the pipes map\n",
    "    pipes[(x, top)] = (x, bottom)\n",
    "    pipes[(x, bottom)] = (x, top)\n",
    "\n",
    "    # start tracking usage of this pipe\n",
    "    pipe_tracker[(x, top)]\n",
    "    pipe_tracker[(x, bottom)]\n",
    "\n",
    "def mark_slide(grid, start_x, start_y, length, direction):\n",
    "    slide_char = '\\\\\\\\' if direction == \"forward\" else '/'\n",
    "    x, y = start_x, start_y\n",
    "\n",
    "    for _ in range(length):\n",
    "        if (x, y) in grid and grid[(x, y)] == 'O':\n",
    "            grid[(x, y)] = slide_char  # replace pegs with slides\n",
    "\n",
    "        # move diagonally in the selected direction\n",
    "        if direction == \"forward\":\n",
    "            x += 1\n",
    "            y -= 1\n",
    "        else:\n",
    "            x -= 1\n",
    "            y -= 1\n",
    "\n",
    "def mark_block(grid, width, row_y):\n",
    "    if row_y in blocks:\n",
    "        return  # skip if already marked\n",
    "\n",
    "    blocks[row_y] = {}  # store original row tiles\n",
    "    for x in range(width):\n",
    "        current_tile = grid.get((x, row_y), ' ')\n",
    "        if current_tile not in {'↥', 'Φ', '⤓', '|'}:  # skip if tile is part of a pipe\n",
    "            blocks[row_y][x] = current_tile  # remember what was here\n",
    "            grid[(x, row_y)] = '█'  # mark block tile\n",
    "\n",
    "def unmark_block(grid, row_y):\n",
    "    if row_y not in blocks:\n",
    "        return  # nothing to unmark\n",
    "\n",
    "    for x, original_char in blocks[row_y].items():\n",
    "        grid[(x, row_y)] = original_char  # restore original tile\n",
    "    del blocks[row_y]  # remove from block tracker\n",
    "\n",
    "def mark_buckets(width, num_buckets):\n",
    "    buckets = {}  # maps x to bucket index\n",
    "    base_size = width // num_buckets  # base size for each bucket\n",
    "    extra = width % num_buckets  # leftover columns\n",
    "    middle_bucket = num_buckets // 2  # middle bucket index\n",
    "    start_x = 0  # starting column for current bucket\n",
    "\n",
    "    for i in range(num_buckets):\n",
    "        # add 1 to size if extra columns remain and it's not the middle bucket\n",
    "        size = base_size + (1 if extra > 0 and i != middle_bucket else 0)\n",
    "        for x in range(start_x, start_x + size):\n",
    "            buckets[x] = i  # map each column to bucket index\n",
    "        start_x += size  # move to next start column\n",
    "        if extra > 0 and i != middle_bucket:\n",
    "            extra -= 1  # use up one extra column\n",
    "\n",
    "    return buckets\n",
    "\n",
    "def visualize_grid(grid, width, height, ball_position=None, buckets=None):\n",
    "    x_labels = \"   \" + \" \".join(str(i % 10) for i in range(width))  # x-axis labels\n",
    "    print(x_labels)  # print top x-axis\n",
    "    for y in range(height - 1, -1, -1):\n",
    "        row = f\"{y:2} \"  # add y-axis label\n",
    "        for x in range(width):\n",
    "            if ball_position and (x, y) == ball_position:\n",
    "                row += 'X'  # draw ball\n",
    "            else:\n",
    "                row += grid.get((x, y), ' ')  # draw tile\n",
    "            row += \" \"\n",
    "        print(row)  # print full row\n",
    "\n",
    "    bucket_row = \"   \"\n",
    "    if buckets:\n",
    "        for x in range(width):\n",
    "            bucket_row += str(buckets.get(x, ' ')) + \" \"\n",
    "    else:\n",
    "        bucket_row += \"  \" * width\n",
    "    print(bucket_row)  # print bucket labels\n",
    "    print(x_labels)  # print bottom x-axis\n",
    "    print(\"===\" + \"=\" * (2 * width))  # draw horizontal divider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Logic: DQN and Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"]) # keep track of action-states for now\n",
    "\n",
    "def choose_action(state, q_table, epsilon, width, grid):\n",
    "    # available actions based on the state type\n",
    "    if isinstance(state[0], str) and state[0] == 'block':\n",
    "        available_actions = list(range(width))\n",
    "    elif isinstance(state[0], tuple): # ledge state\n",
    "        ledge_start_x, ledge_y = state[0]\n",
    "        # check if ledge_y is valid before accessing grid\n",
    "        available_actions = [col for col in range(width) if grid.get((col, ledge_y)) in {'_', '⬒', '⤓', '↥'}]\n",
    "        if not available_actions:\n",
    "           available_actions = [col for col in range(width) if grid.get((col, ledge_y)) in {'_', '⬒', '⤓', '↥'}]\n",
    "    else:\n",
    "        print(f\"Warning: Unrecognized state format for action selection: {state}\")\n",
    "        available_actions = list(range(width))\n",
    "        \n",
    "    # check state exists in the q_table (initialize if not)\n",
    "    if state not in q_table:\n",
    "        q_table[state] = defaultdict(float)\n",
    "        for act in available_actions:\n",
    "             q_table[state][act] = 0.0 \n",
    "    current_q_actions = q_table[state]\n",
    "    for act in available_actions:\n",
    "        if act not in current_q_actions:\n",
    "             current_q_actions[act] = 0.0\n",
    "    \n",
    "    # if no actions were available or initialized, fallback\n",
    "    if not available_actions:\n",
    "         print(f\"Error: No available actions determined for state {state}. Choosing random column.\")\n",
    "         return random.choice(list(range(width)))\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(available_actions)  # explore\n",
    "    else:\n",
    "        # choose the action with the highest Q-value from available actions\n",
    "        q_values = q_table[state]\n",
    "        max_q = -float('inf')\n",
    "        best_actions = []\n",
    "        \n",
    "        for act in available_actions: \n",
    "            q_val = q_values.get(act, 0.0) # set to 0 if action not seen before in this state\n",
    "            if q_val > max_q:\n",
    "                max_q = q_val\n",
    "                best_actions = [act]\n",
    "            elif q_val == max_q:\n",
    "                best_actions.append(act)\n",
    "        \n",
    "        if not best_actions: # if all available actions have Q=-inf\n",
    "             return random.choice(available_actions) # select random available action\n",
    "             \n",
    "        return random.choice(best_actions) # choose randomly among best actions\n",
    "\n",
    "# find the state key ((start_x, y), frozenset(buttons)) for the ledge the ball is currently on\n",
    "def find_ledge_state_key(x, y, width, grid, pressed_buttons):\n",
    "    # search left from current x to find the start of the connected ledge segment\n",
    "    ledge_start_x = x\n",
    "    while ledge_start_x >= 0 and grid.get((ledge_start_x, y)) in {'_', '⬒', '⤓', '↥'}:\n",
    "        ledge_start_x -= 1\n",
    "    ledge_start_x += 1 # correct start position\n",
    "    \n",
    "    # check if the found start is actually a ledge tile\n",
    "    if grid.get((ledge_start_x, y)) in {'_', '⬒', '⤓', '↥'}:\n",
    "         return ((ledge_start_x, y), frozenset(pressed_buttons))\n",
    "    else:\n",
    "         # this is when the ball is on a pipe tile adjacent to a ledge and not technically on the ledge start itself\n",
    "         # return None if we can't confirm the start point\n",
    "         print(f\"Debug: Could not confirm ledge start for ({x},{y}). Found start_x={ledge_start_x}, tile={grid.get((ledge_start_x, y))}\")\n",
    "         return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-Learning and Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDQN learning initialization\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99 # higher discount factor for potentially long paths\n",
    "exploration_rate = 1.0  # start fully exploratory\n",
    "exploration_decay = 0.995  # slow decay\n",
    "min_exploration = 0.01  # smallest possible exploration rate\n",
    "episodes = 1000  # number of training episodes\n",
    "\n",
    "update_frequency = 4 # learn every 4 steps\n",
    "target_update_frequency = 100 # update target table every 100 steps\n",
    "soft_update_alpha = 0.01 # soft update parameter\n",
    "\n",
    "# training setup\n",
    "target_bucket = 1  # the buckets the agent should aim for\n",
    "total_timesteps = 0 # counter for triggering learn/target updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(grid, width, learning_rate, discount_factor):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return # not enough samples yet\n",
    "\n",
    "    mini_batch = random.sample(replay_buffer, batch_size)\n",
    "\n",
    "    for experience in mini_batch:\n",
    "        state, action, reward, next_state, done = experience\n",
    "        # check state and action exist in online table\n",
    "        if state not in q_table_online or action not in q_table_online[state]:\n",
    "             q_table_online[state][action] = 0.0\n",
    "        if state not in q_table_target or action not in q_table_target[state]:\n",
    "             q_table_target[state][action] = 0.0\n",
    "            \n",
    "        current_q_online = q_table_online[state][action]\n",
    "\n",
    "        # target calculation\n",
    "        if done:\n",
    "            target_q = reward # use the final reward directly\n",
    "        else:\n",
    "            next_state_type = next_state[0]\n",
    "            if isinstance(next_state_type, str) and next_state_type == 'block':\n",
    "                next_available_actions = list(range(width))\n",
    "            elif isinstance(next_state_type, tuple):\n",
    "                ledge_y_next = next_state_type[1]\n",
    "                next_available_actions = [col for col in range(width) if grid.get((col, ledge_y_next)) in {'_', '⬒', '⤓', '↥'}]\n",
    "            else:\n",
    "                print(f\"Error: Unknown next state format in learn: {next_state}\")\n",
    "                next_available_actions = []\n",
    "                \n",
    "            if next_state not in q_table_online:\n",
    "                 q_table_online[next_state] = defaultdict(float)\n",
    "                 for act in next_available_actions:\n",
    "                    q_table_online[next_state][act] = 0.0\n",
    "            if next_state not in q_table_target:\n",
    "                 q_table_target[next_state] = defaultdict(float)\n",
    "                 for act in next_available_actions:\n",
    "                    q_table_target[next_state][act] = 0.0\n",
    "            \n",
    "            # best action in next state using online table\n",
    "            online_q_next = q_table_online[next_state]\n",
    "            if not online_q_next or not next_available_actions: # if no actions available or state just initialized\n",
    "                 best_next_action = None\n",
    "                 max_online_q_next = 0.0\n",
    "            else:\n",
    "                 max_online_q_next = -float('inf')\n",
    "                 best_actions_next = []\n",
    "                 for act in next_available_actions: # check only valid next actions\n",
    "                    q_val = online_q_next.get(act, 0.0) \n",
    "                    if q_val > max_online_q_next:\n",
    "                        max_online_q_next = q_val\n",
    "                        best_actions_next = [act]\n",
    "                    elif q_val == max_online_q_next:\n",
    "                        best_actions_next.append(act)\n",
    "                 if not best_actions_next:\n",
    "                      best_next_action = random.choice(next_available_actions)\n",
    "                 else:\n",
    "                     best_next_action = random.choice(best_actions_next)\n",
    "\n",
    "            # get Q-value for that best action using target table\n",
    "            q_target_next = q_table_target[next_state].get(best_next_action, 0.0) # 0 if action missing\n",
    "                 \n",
    "            # reward for intermediate steps is 0\n",
    "            target_q = discount_factor * q_target_next # R_i=0 + gamma * Q_target(s', argmax Q_online(s',a'))\n",
    "        # update online Q-table\n",
    "        q_table_online[state][action] = current_q_online + learning_rate * (target_q - current_q_online)\n",
    "\n",
    "# target table update        \n",
    "def update_target_network(online_q_table, target_q_table, alpha):\n",
    "    for state in online_q_table:\n",
    "        if state not in target_q_table:\n",
    "             target_q_table[state] = defaultdict(float)\n",
    "             \n",
    "        for action in online_q_table[state]:\n",
    "            # initialize target action value if it doesn't exist\n",
    "            if action not in target_q_table[state]:\n",
    "                 target_q_table[state][action] = 0.0 \n",
    "            target_q_table[state][action] = (1.0 - alpha) * target_q_table[state][action] + alpha * online_q_table[state][action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_ball_episode(grid, width, height, start_x, buckets, exploration_rate, target_bucket, should_learn):\n",
    "    x, y = start_x, height - 1\n",
    "    pressed_buttons = set()\n",
    "    last_state = None\n",
    "    last_action = -1\n",
    "    time_step = 0 # keep track of steps\n",
    "    global total_decision_steps\n",
    "    # visualize_grid(grid, width, height, None, buckets) # can be slow\n",
    "\n",
    "    while y > 0:\n",
    "        # visualize_grid(grid, width, height, (x,y), buckets)\n",
    "        current_tile = grid.get((x, y))\n",
    "        current_state_tuple = None\n",
    "        \n",
    "        # check for decision points, ledges or blocks\n",
    "        is_ledge = current_tile in {'_', '⬒', '⤓', '↥'}\n",
    "        is_block = current_tile == '█' or (current_tile in {'⤓', '↥'} and (grid.get((x - 1, y), '') == '█' or grid.get((x + 1, y), '') == '█'))\n",
    "        if is_ledge or is_block:\n",
    "            # find state\n",
    "            current_pressed_buttons_frozen = frozenset(pressed_buttons)\n",
    "            if is_ledge:\n",
    "                state_key_info = find_ledge_state_key(x, y, width, grid, current_pressed_buttons_frozen)\n",
    "                if state_key_info:\n",
    "                    current_state_tuple = state_key_info\n",
    "                    ledge_tracker[current_state_tuple] += 1\n",
    "                else:\n",
    "                    print(f\"Error: Ledge state key not found at ({x},{y}) tile '{current_tile}'\")\n",
    "                    # treat as terminal error state for this episode\n",
    "                    reward = -50.0 \n",
    "                    done = True\n",
    "                    if last_state is not None: # save final error transition\n",
    "                       replay_buffer.append(Experience(last_state, last_action, reward, None, done))\n",
    "                    break\n",
    "            elif is_block:\n",
    "                current_state_tuple = (('block', y), current_pressed_buttons_frozen)\n",
    "                block_row_tracker[current_state_tuple] += 1\n",
    "                \n",
    "            # save previous transition\n",
    "            if last_state is not None:\n",
    "                # the reward for intermediate steps is 0 for now\n",
    "                replay_buffer.append(Experience(last_state, last_action, 0, current_state_tuple, False))\n",
    "                if should_learn() and len(replay_buffer) >= batch_size:\n",
    "                    learn(grid, width, learning_rate, discount_factor)\n",
    "                    if total_decision_steps % target_update_frequency == 0:\n",
    "                        update_target_network(q_table_online, q_table_target, soft_update_alpha)\n",
    "         \n",
    "            # select action\n",
    "            action = choose_action(current_state_tuple, q_table_online, exploration_rate, width, grid)\n",
    "            last_state = current_state_tuple\n",
    "            last_action = action\n",
    "            time_step += 1\n",
    "            total_decision_steps += 1\n",
    "            done = False  # ensures 'done' is always defined\n",
    "            \n",
    "            # next action of button, pipe, or just moving x\n",
    "            chosen_tile = grid.get((action, y))\n",
    "            if chosen_tile == '⬒':\n",
    "                grid[(action, y)] = '_' \n",
    "                unmark_block(grid, 5) \n",
    "                button_tracker[(action, y)] += 1\n",
    "                pressed_buttons.add((action, y))\n",
    "                x = action # move to the button's column or stay\n",
    "                continue \n",
    "            elif (action, y) in pipes:\n",
    "                pipe_tracker[(action, y)] += 1\n",
    "                x, y = pipes[(action, y)]\n",
    "                continue\n",
    "            else:\n",
    "                x = action # normal drop off, move to chosen column\n",
    "                # fall straight down if empty space below\n",
    "                while (x, y - 1) in grid and grid[(x, y - 1)] == ' ':\n",
    "                    y -= 1\n",
    "                y -= 1 # drop one more row \n",
    "                continue\n",
    "        \n",
    "        # slide tiles\n",
    "        if current_tile in {'\\\\\\\\\\\\\\\\', '/'}:\n",
    "            x += 1 if current_tile == '\\\\\\\\\\\\\\\\' else -1\n",
    "            y -= 1\n",
    "            # boundary check after slide move\n",
    "            if not (0 <= x < width and 0 <= y < height):\n",
    "                y = 0 # falling off bottom\n",
    "                break\n",
    "            continue\n",
    "            \n",
    "        # spike check\n",
    "        if current_tile == '^':\n",
    "            if y in spike_tracker:\n",
    "                spike_tracker[y] += 1\n",
    "            reward = 0 \n",
    "            done = True\n",
    "            if last_state is not None:\n",
    "                 replay_buffer.append(Experience(last_state, last_action, reward, None, done))\n",
    "                 if should_learn() and len(replay_buffer) >= batch_size:\n",
    "                    learn(grid, width, learning_rate, discount_factor)\n",
    "                    if total_decision_steps % target_update_frequency == 0:\n",
    "                        update_target_network(q_table_online, q_table_target, soft_update_alpha)\n",
    "            break\n",
    "\n",
    "        # try falling diagonally\n",
    "        possible_moves = []\n",
    "        left_diag_coord = (x - 1, y - 1)\n",
    "        right_diag_coord = (x + 1, y - 1)\n",
    "\n",
    "        # check if left diagonal is valid (on board and not empty space ' ')\n",
    "        if 0 <= left_diag_coord[0] < width and 0 <= left_diag_coord[1] < height and grid.get(left_diag_coord) != ' ':\n",
    "            possible_moves.append(left_diag_coord)\n",
    "        \n",
    "        # check if right diagonal is valid\n",
    "        if 0 <= right_diag_coord[0] < width and 0 <= right_diag_coord[1] < height and grid.get(right_diag_coord) != ' ':\n",
    "            possible_moves.append(right_diag_coord)\n",
    "            \n",
    "        if possible_moves:\n",
    "            chosen_move = random.choice(possible_moves)\n",
    "            x, y = chosen_move\n",
    "        else:\n",
    "            # fall straight down if no diagonal options\n",
    "            y -= 1\n",
    "\n",
    "    # end of episode\n",
    "    final_bucket = -1\n",
    "    if not done: # when loop exited because y <= 0\n",
    "        done = True\n",
    "        final_bucket = buckets.get(x, -1) # -1 if out of bounds\n",
    "        if final_bucket == target_bucket:\n",
    "            reward = 1.0\n",
    "        elif final_bucket != -1:\n",
    "            reward = -1.0\n",
    "        else:\n",
    "             reward = -1.0\n",
    "        \n",
    "        if final_bucket != -1:\n",
    "            bucket_tracker[final_bucket] += 1\n",
    "            \n",
    "        # save final transition\n",
    "        if last_state is not None:\n",
    "            replay_buffer.append(Experience(last_state, last_action, reward, None, done))\n",
    "            if should_learn() and len(replay_buffer) >= batch_size:\n",
    "                learn(grid, width, learning_rate, discount_factor)\n",
    "                if total_decision_steps % target_update_frequency == 0:\n",
    "                    update_target_network(q_table_online, q_table_target, soft_update_alpha)\n",
    "    \n",
    "    return reward, final_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Board Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_board():\n",
    "    width, height = 15, 30\n",
    "    num_buckets = 5\n",
    "    grid = generate_grid(width, height)\n",
    "    # reset global blocks for the new board\n",
    "    global blocks\n",
    "    blocks = {}\n",
    "    # reset pipes for the new board\n",
    "    global pipes\n",
    "    pipes = {}\n",
    "\n",
    "    mark_ledge(grid, start_x=2, length=5, ledge_y=27)\n",
    "    mark_ledge(grid, start_x=6, length=4, ledge_y=24)\n",
    "    mark_ledge(grid, start_x=1, length=6, ledge_y=21)\n",
    "    mark_ledge(grid, start_x=9, length=5, ledge_y=19)\n",
    "    mark_ledge(grid, start_x=3, length=7, ledge_y=17)\n",
    "    mark_ledge(grid, start_x=7, length=6, ledge_y=15)\n",
    "    mark_ledge(grid, start_x=0, length=5, ledge_y=13)\n",
    "    mark_ledge(grid, start_x=9, length=6, ledge_y=11)\n",
    "    mark_ledge(grid, start_x=0, length=9, ledge_y=9, button_x=0)\n",
    "    mark_ledge(grid, start_x=5, length=6, ledge_y=7)\n",
    "    mark_ledge(grid, start_x=4, length=5, ledge_y=5)\n",
    "    mark_ledge(grid, start_x=8, length=4, ledge_y=3)\n",
    "    mark_ledge(grid, start_x=4, length=8, ledge_y=2)\n",
    "\n",
    "    mark_slide(grid, start_x=0, start_y=28, length=4, direction=\"forward\")\n",
    "    mark_slide(grid, start_x=13, start_y=23, length=3, direction=\"backward\")\n",
    "    mark_slide(grid, start_x=14, start_y=16, length=4, direction=\"backward\")\n",
    "    mark_slide(grid, start_x=0, start_y=12, length=2, direction=\"forward\")\n",
    "\n",
    "    mark_spike(grid, start_x=5, length=4, spike_y=18)\n",
    "    mark_spike(grid, start_x=7, length=2, spike_y=6)\n",
    "\n",
    "    mark_pipe(grid, x=6, y1=27, y2=24)\n",
    "    mark_pipe(grid, x=9, y1=24, y2=19)\n",
    "    mark_pipe(grid, x=4, y1=9, y2=5)\n",
    "\n",
    "    mark_block(grid, width, row_y=5)\n",
    "    buckets = mark_buckets(width, num_buckets)\n",
    "\n",
    "    return grid, buckets, width, height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop (DDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset trackers for this training run\n",
    "bucket_tracker = {i: 0 for i in range(5)} \n",
    "ledge_tracker = defaultdict(int)  \n",
    "block_row_tracker = defaultdict(int) \n",
    "spike_tracker = defaultdict(int)  \n",
    "pipe_tracker = defaultdict(int) \n",
    "button_tracker = defaultdict(int) \n",
    "episode_rewards_history = [] \n",
    "most_recent_rewards = deque(maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Avg Reward (Last 100): -0.66 | Exploration Rate: 0.01 | Buffer Size: 10000 | Q-States: 10\n",
      "Episode 200 | Avg Reward (Last 100): -0.32 | Exploration Rate: 0.01 | Buffer Size: 10000 | Q-States: 10\n",
      "Episode 300 | Avg Reward (Last 100): -0.36 | Exploration Rate: 0.01 | Buffer Size: 10000 | Q-States: 10\n",
      "Episode 400 | Avg Reward (Last 100): -0.68 | Exploration Rate: 0.01 | Buffer Size: 10000 | Q-States: 10\n",
      "Episode 500 | Avg Reward (Last 100): -0.60 | Exploration Rate: 0.01 | Buffer Size: 10000 | Q-States: 10\n",
      "Episode 600 | Avg Reward (Last 100): -0.46 | Exploration Rate: 0.01 | Buffer Size: 10000 | Q-States: 10\n",
      "Episode 700 | Avg Reward (Last 100): -0.70 | Exploration Rate: 0.01 | Buffer Size: 10000 | Q-States: 10\n",
      "Episode 800 | Avg Reward (Last 100): -0.56 | Exploration Rate: 0.01 | Buffer Size: 10000 | Q-States: 10\n",
      "Episode 900 | Avg Reward (Last 100): -0.62 | Exploration Rate: 0.01 | Buffer Size: 10000 | Q-States: 10\n",
      "Episode 1000 | Avg Reward (Last 100): -0.36 | Exploration Rate: 0.01 | Buffer Size: 10000 | Q-States: 10\n",
      "\n",
      "Ledge State Visit Statistics (Top 15):\n",
      "State ((4, 9), frozenset()) visited 835147 times\n",
      "State ((4, 5), frozenset()) visited 208298 times\n",
      "State ((4, 5), frozenset({(0, 9)})) visited 40645 times\n",
      "State ((4, 9), frozenset({(0, 9)})) visited 39794 times\n",
      "State ((0, 9), frozenset({(0, 9)})) visited 2000 times\n",
      "State ((9, 19), frozenset()) visited 716 times\n",
      "State ((6, 24), frozenset()) visited 339 times\n",
      "State ((9, 24), frozenset()) visited 220 times\n",
      "State ((6, 27), frozenset()) visited 162 times\n",
      "\n",
      "Block Row State Visit Statistics (Top 15):\n",
      "State (('block', 5), frozenset()) visited 626849 times\n",
      "\n",
      "Bucket Landing Statistics:\n",
      "Bucket 0: 468 landings (23.4%)\n",
      "Bucket 1: 445 landings (22.2%)\n",
      "Bucket 2: 852 landings (42.6%)\n",
      "Bucket 3: 177 landings (8.8%)\n",
      "Bucket 4: 58 landings (2.9%)\n",
      "\n",
      "Spike Hit Statistics:\n",
      "\n",
      "Pipe Usage Statistics:\n",
      "Pipe at (4, 5) used 874941 times\n",
      "Pipe at (4, 9) used 248943 times\n",
      "Pipe at (6, 24) used 162 times\n",
      "Pipe at (6, 27) used 49 times\n",
      "Pipe at (9, 19) used 220 times\n",
      "Pipe at (9, 24) used 210 times\n",
      "\n",
      "Button Press Statistics:\n",
      "Button at (0, 9) pressed 2000 times\n",
      "\n",
      "Online Q-Table (Sample):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "('position', 'buttons_pressed')",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "0",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "12",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d541694b-d9ee-4a44-a0b1-f881e79f3250",
       "rows": [
        [
         "((9, 19), ())",
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "(('block', 5), ())",
         null,
         null,
         "0.0",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "((4, 9), ())",
         "-0.3289176158592797",
         "0.0",
         "0.0",
         "0.0",
         null,
         "0.0",
         null,
         null,
         null
        ],
        [
         "((4, 5), ())",
         null,
         null,
         "0.0",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "((0, 9), ((0, 9),))",
         "-0.9548724628848761",
         "-0.9999977084642213",
         "-0.3227910601206301",
         "-0.9999053949734287",
         null,
         "-0.9994147526730557",
         null,
         null,
         null
        ],
        [
         "((4, 5), ((0, 9),))",
         null,
         null,
         "-0.34877125462007685",
         "-0.9566020584205185",
         null,
         "-0.9622488475240106",
         null,
         null,
         null
        ],
        [
         "((9, 24), ())",
         null,
         null,
         null,
         "0.0",
         "0.0",
         null,
         "0.0",
         null,
         null
        ],
        [
         "((6, 27), ())",
         null,
         "0.0",
         "0.0",
         "0.0",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "((6, 24), ())",
         null,
         null,
         null,
         "0.0",
         "0.0",
         null,
         "0.0",
         null,
         null
        ],
        [
         "((4, 9), ((0, 9),))",
         "-0.9999650566117262",
         "-0.8308447062261556",
         "-0.3255467929276832",
         "-0.6449822981743",
         null,
         "-0.999999802996445",
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>position</th>\n",
       "      <th>buttons_pressed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(9, 19)</th>\n",
       "      <th>()</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(block, 5)</th>\n",
       "      <th>()</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4, 9)</th>\n",
       "      <th>()</th>\n",
       "      <td>-0.328918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4, 5)</th>\n",
       "      <th>()</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, 9)</th>\n",
       "      <th>((0, 9),)</th>\n",
       "      <td>-0.954872</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.322791</td>\n",
       "      <td>-0.999905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.999415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4, 5)</th>\n",
       "      <th>((0, 9),)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.348771</td>\n",
       "      <td>-0.956602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.962249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9, 24)</th>\n",
       "      <th>()</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(6, 27)</th>\n",
       "      <th>()</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(6, 24)</th>\n",
       "      <th>()</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4, 9)</th>\n",
       "      <th>((0, 9),)</th>\n",
       "      <td>-0.999965</td>\n",
       "      <td>-0.830845</td>\n",
       "      <td>-0.325547</td>\n",
       "      <td>-0.644982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0         2         4         6    7   \\\n",
       "position   buttons_pressed                                                \n",
       "(9, 19)    ()                    NaN       NaN       NaN       NaN  NaN   \n",
       "(block, 5) ()                    NaN       NaN  0.000000       NaN  NaN   \n",
       "(4, 9)     ()              -0.328918  0.000000  0.000000  0.000000  NaN   \n",
       "(4, 5)     ()                    NaN       NaN  0.000000       NaN  NaN   \n",
       "(0, 9)     ((0, 9),)       -0.954872 -0.999998 -0.322791 -0.999905  NaN   \n",
       "(4, 5)     ((0, 9),)             NaN       NaN -0.348771 -0.956602  NaN   \n",
       "(9, 24)    ()                    NaN       NaN       NaN  0.000000  0.0   \n",
       "(6, 27)    ()                    NaN  0.000000  0.000000  0.000000  NaN   \n",
       "(6, 24)    ()                    NaN       NaN       NaN  0.000000  0.0   \n",
       "(4, 9)     ((0, 9),)       -0.999965 -0.830845 -0.325547 -0.644982  NaN   \n",
       "\n",
       "                                  8    9    10   12  \n",
       "position   buttons_pressed                           \n",
       "(9, 19)    ()                    NaN  0.0  0.0  0.0  \n",
       "(block, 5) ()                    NaN  NaN  NaN  NaN  \n",
       "(4, 9)     ()               0.000000  NaN  NaN  NaN  \n",
       "(4, 5)     ()                    NaN  NaN  NaN  NaN  \n",
       "(0, 9)     ((0, 9),)       -0.999415  NaN  NaN  NaN  \n",
       "(4, 5)     ((0, 9),)       -0.962249  NaN  NaN  NaN  \n",
       "(9, 24)    ()                    NaN  0.0  NaN  NaN  \n",
       "(6, 27)    ()                    NaN  NaN  NaN  NaN  \n",
       "(6, 24)    ()                    NaN  0.0  NaN  NaN  \n",
       "(4, 9)     ((0, 9),)       -1.000000  NaN  NaN  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_decision_steps = 0\n",
    "steps_after_episode = 0\n",
    "# synchronize Q-tables initially\n",
    "q_table_target = copy.deepcopy(q_table_online) \n",
    " \n",
    "# agent determines when to call learn() based on total steps\n",
    "def should_learn():\n",
    "     return total_decision_steps % update_frequency == 0\n",
    "\n",
    "# training loop\n",
    "for episode in range(episodes):\n",
    "    grid, buckets, width, height = build_board()\n",
    "    if (0,9) in grid and grid[(0,9)] == '_': \n",
    "        grid[(0,9)] = '⬒'\n",
    "    if 5 not in blocks:\n",
    "         mark_block(grid, width, 5)\n",
    "         \n",
    "    start_x = random.randint(0, width - 1)\n",
    "    \n",
    "    # drop_ball_episode now handles simulation and storing experiences\n",
    "    # calls learn() internally based on the trigger function\n",
    "    episode_final_reward, final_bucket = drop_ball_episode(grid, width, height, start_x, buckets, exploration_rate, target_bucket, should_learn)\n",
    "\n",
    "    steps_after_episode += total_decision_steps\n",
    "    \n",
    "    # check if target table update is due based on step count progression\n",
    "    if (total_decision_steps // target_update_frequency) < (steps_after_episode // target_update_frequency):\n",
    "       update_target_network(q_table_online, q_table_target, soft_update_alpha)\n",
    "       # print(f\"Target network updated after episode {episode + 1}\")\n",
    "    \n",
    "    # perform learning steps from replay buffer\n",
    "    if len(replay_buffer) > batch_size:\n",
    "        for _ in range(10): # example 10 learning steps per episode end\n",
    "             learn(grid, width, learning_rate, discount_factor) \n",
    "             \n",
    "    episode_rewards_history.append(episode_final_reward) # save the final reward\n",
    "    most_recent_rewards.append(episode_final_reward)\n",
    "    \n",
    "    # decay exploration rate\n",
    "    exploration_rate = max(min_exploration, exploration_rate * exploration_decay)\n",
    "\n",
    "    # print progress\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        avg_reward = sum(most_recent_rewards) / len(most_recent_rewards)\n",
    "        print(f\"Episode {episode + 1} | Avg Reward (Last 100): {avg_reward:.2f} | Exploration Rate: {exploration_rate:.2f} | Buffer Size: {len(replay_buffer)} | Q-States: {len(q_table_online)}\")\n",
    "\n",
    "# final statistics\n",
    "print(\"\\nLedge State Visit Statistics (Top 15):\")\n",
    "sorted_ledges = sorted(ledge_tracker.items(), key=lambda item: item[1], reverse=True)\n",
    "for state, count in sorted_ledges[:15]:\n",
    "    print(f\"State {state} visited {count} times\")\n",
    "\n",
    "print(\"\\nBlock Row State Visit Statistics (Top 15):\")\n",
    "sorted_blocks = sorted(block_row_tracker.items(), key=lambda item: item[1], reverse=True)\n",
    "for state, count in sorted_blocks[:15]:\n",
    "    print(f\"State {state} visited {count} times\")\n",
    "\n",
    "print(\"\\nBucket Landing Statistics:\")\n",
    "total_landings = sum(bucket_tracker.values())\n",
    "for bucket_id, count in sorted(bucket_tracker.items()):\n",
    "    if count > 0:\n",
    "        percent = (count / total_landings * 100) if total_landings > 0 else 0\n",
    "        print(f\"Bucket {bucket_id}: {count} landings ({percent:.1f}%)\")\n",
    "\n",
    "print(\"\\nSpike Hit Statistics:\")\n",
    "for y, count in sorted(spike_tracker.items()):\n",
    "     if count > 0:\n",
    "        print(f\"Spike row {y} hit {count} times\")\n",
    "\n",
    "print(\"\\nPipe Usage Statistics:\")\n",
    "for (x, y), count in sorted(pipe_tracker.items()):\n",
    "    if count > 0:\n",
    "        print(f\"Pipe at ({x}, {y}) used {count} times\")\n",
    "\n",
    "print(\"\\nButton Press Statistics:\")\n",
    "for (x, y), count in sorted(button_tracker.items()):\n",
    "    if count > 0:\n",
    "        print(f\"Button at ({x}, {y}) pressed {count} times\")\n",
    "\n",
    "# show online Q-table\n",
    "q_data_for_df = {}\n",
    "for state, actions in q_table_online.items():\n",
    "    state_display = (state[0], tuple(sorted(list(state[1]))))\n",
    "    q_data_for_df[state_display] = actions\n",
    "\n",
    "q_df = pd.DataFrame(q_data_for_df).T\n",
    "\n",
    "if not q_df.empty:\n",
    "    q_df.index = pd.MultiIndex.from_tuples(\n",
    "        q_df.index,\n",
    "        names=[\"position\", \"buttons_pressed\"]\n",
    "    )\n",
    "    \n",
    "    def get_y_sort_key(idx):\n",
    "        y_keys = []\n",
    "        for pos, buttons in idx:\n",
    "            if isinstance(pos, tuple) and len(pos) == 2 and isinstance(pos[1], int):\n",
    "                y_keys.append(-pos[1])\n",
    "            elif isinstance(pos, tuple) and len(pos) == 2 and pos[0] == 'block':\n",
    "                 y_keys.append(-pos[1])\n",
    "            else:\n",
    "                y_keys.append(0) \n",
    "        return y_keys\n",
    "        \n",
    "    q_df = q_df.sort_index(level=0, key=get_y_sort_key, sort_remaining=False)\n",
    "    q_df = q_df.reindex(sorted(q_df.columns), axis=1)\n",
    "\n",
    "    print(\"\\nOnline Q-Table (Sample):\")\n",
    "    display(q_df.head(20)) \n",
    "else:\n",
    "    print(\"\\nQ-Table is empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
