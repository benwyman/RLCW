{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the Plinko DQN algorithm using Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict, deque\n",
    "import copy # deep copying Q-table\n",
    "from collections import namedtuple\n",
    "from state_utils_deep import drop_ball, initialize_trackers\n",
    "from board_builder import build_board\n",
    "from visualization import print_training_stats, visualize_grid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Double Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation: \n",
    ">The original Plinko code uses standard Q-learning. Q-learning is known for maximization bias, leading to overestimation of action values. Our standard Q-learning algorithm uses one Q-table to select both the best next action and to evaluate the value of that action. If some action's value is overestimated our max operation will likely select it therefore distributing the overestimation. Double Q-learning ensures that our selection and evaluation are separate. We will use the online Q-table to select the best next action while using the target Q-table to evaluate the value of that chosen action. This will reduce the chance of consistently selecting actions based on overestimated values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation: \n",
    ">We expect more accurate Q-value estimates, which will hopefully result in a more stable learning process and convergence to a better final policy to ensure a higher success rate for the target bucket. It might also prevent our agent from getting stuck favouring sub-optimal paths due to early overestimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc3(torch.relu(self.fc2(torch.relu(self.fc1(x)))))\n",
    "\n",
    "def encode_state(state, width):\n",
    "    # example encoding: [is_block (0/1)] + y + buttons (one-hot for up to N buttons)\n",
    "    if isinstance(state[0], tuple):  # ledge\n",
    "        x, y = state[0]\n",
    "        button_vector = torch.zeros(width * 5)  # assuming max 5 buttons, adjust as needed\n",
    "        for bx, by in state[1]:\n",
    "            index = (by * width + bx) % (width * 5)\n",
    "            button_vector[index] = 1\n",
    "        return torch.tensor([0, x, y], dtype=torch.float32).unsqueeze(0), button_vector.unsqueeze(0)\n",
    "    elif isinstance(state[0], str):  # block\n",
    "        y = state[0][1]\n",
    "        button_vector = torch.zeros(width * 5)\n",
    "        for bx, by in state[1]:\n",
    "            index = (by * width + bx) % (width * 5)\n",
    "            button_vector[index] = 1\n",
    "        return torch.tensor([1, y], dtype=torch.float32).unsqueeze(0), button_vector.unsqueeze(0)\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized state format\")\n",
    "\n",
    "def preprocess_state(state, width):\n",
    "    base, buttons = encode_state(state, width)\n",
    "    return torch.cat([base, buttons], dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-Learning and Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaration of tracker dictionaries\n",
    "trackers = initialize_trackers()\n",
    "\n",
    "# DDQN learning initialization\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99 # higher discount factor for potentially long paths\n",
    "exploration_rate = 1.0  # start fully exploratory\n",
    "exploration_decay = 0.999  # slow decay\n",
    "min_exploration = 0.01  # smallest possible exploration rate\n",
    "episodes = 1000  # number of training episodes\n",
    "initial_free_exploration = 100\n",
    "\n",
    "update_frequency = 4 # learn every 4 steps\n",
    "target_update_frequency = 100 # update target table every 100 steps\n",
    "soft_update_alpha = 0.1 # soft update parameter\n",
    "\n",
    "# Experience Replay\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "replay_buffer = deque(maxlen=10000) # store last 10k transitions\n",
    "batch_size = 64\n",
    "\n",
    "# training setup\n",
    "target_bucket = 4  # the bucket the agent should aim for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop (DDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def learn(grid, width, learning_rate, discount_factor):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "\n",
    "    state_batch = torch.cat([preprocess_state(s, width) for s, _, _, _, _ in batch])\n",
    "    action_batch = torch.tensor([a for _, a, _, _, _ in batch], dtype=torch.int64).unsqueeze(1)\n",
    "    reward_batch = torch.tensor([r for _, _, r, _, _ in batch], dtype=torch.float32).unsqueeze(1)\n",
    "    done_batch = torch.tensor([d for _, _, _, _, d in batch], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    next_states = [s for _, _, _, s, _ in batch]\n",
    "    non_final_mask = torch.tensor([s is not None for s in next_states], dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([preprocess_state(s, width) for s in next_states if s is not None])\n",
    "\n",
    "    q_values = online_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    target_q_values = torch.zeros(batch_size, 1)\n",
    "    if non_final_next_states.size(0) > 0:\n",
    "        next_actions = online_net(non_final_next_states).argmax(dim=1).unsqueeze(1)\n",
    "        target_q = target_net(non_final_next_states).gather(1, next_actions)\n",
    "        target_q_values[non_final_mask] = target_q.detach()\n",
    "\n",
    "    expected_q = reward_batch + discount_factor * target_q_values * (1 - done_batch)\n",
    "\n",
    "    loss = loss_fn(q_values, expected_q)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()  # <-- return the loss\n",
    "\n",
    "def update_target_network(online_model, target_model, alpha):\n",
    "    for target_param, online_param in zip(target_model.parameters(), online_model.parameters()):\n",
    "        target_param.data.copy_(alpha * online_param.data + (1 - alpha) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_decision_steps = [0]\n",
    "steps_after_episode = 0\n",
    "episode_rewards_history = []\n",
    "most_recent_rewards = deque(maxlen=100)\n",
    "total_stars_collected = 0\n",
    "\n",
    "\n",
    "\n",
    "grid, buckets, width, height = build_board(\"hard\", trackers)\n",
    "\n",
    "# Q-Learning Specific\n",
    "# Two Q-tables for Double DQN\n",
    "input_dim = 3 + (width * 5)  # 3 for [type, x, y] or [type, y], rest for button vector\n",
    "output_dim = width  # one output per column choice\n",
    "\n",
    "online_net = QNetwork(input_dim, output_dim)\n",
    "target_net = QNetwork(input_dim, output_dim)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(online_net.parameters(), lr=0.001)\n",
    "\n",
    "start_x = random.randint(0, width - 1)\n",
    "\n",
    "visualize_grid(grid, width, height, ball_position=(start_x, height - 1), buckets=buckets)\n",
    "\n",
    "# agent determines when to call learn() based on total steps\n",
    "def should_learn():\n",
    "     return total_decision_steps[0] % update_frequency == 0\n",
    "\n",
    "# training loop\n",
    "for episode in range(episodes):\n",
    "    grid, buckets, width, height = build_board(\"hard\", trackers)\n",
    "         \n",
    "    start_x = random.randint(0, width - 1)\n",
    "    \n",
    "    episode_final_reward, final_bucket, stars_collected = drop_ball(\n",
    "        grid=grid,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        start_x=start_x,\n",
    "        buckets=buckets,\n",
    "        target_bucket=target_bucket,\n",
    "        exploration_rate=exploration_rate,\n",
    "        q_model=online_net,\n",
    "        trackers=trackers,\n",
    "        extra={\n",
    "            \"replay_buffer\": replay_buffer,\n",
    "            \"should_learn\": should_learn,\n",
    "            \"learn\": learn,\n",
    "            \"Experience\": Experience,\n",
    "            \"target_net\": target_net,\n",
    "            \"soft_update_alpha\": soft_update_alpha,\n",
    "            \"update_target_network\": update_target_network,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"discount_factor\": discount_factor,\n",
    "            \"total_decision_steps\": total_decision_steps,  # pass as list for mutability\n",
    "            \"target_update_frequency\": target_update_frequency\n",
    "        },\n",
    "        # visualize=(episode == episodes - 3)\n",
    "        visualize=False\n",
    "    )\n",
    "    steps_after_episode += total_decision_steps[0]\n",
    "    total_stars_collected += len(stars_collected)\n",
    "    \n",
    "    # perform learning steps from replay buffer\n",
    "    losses = []\n",
    "    if len(replay_buffer) > batch_size:\n",
    "        for _ in range(10): # example 10 learning steps per episode end\n",
    "            loss = learn(grid, width, learning_rate, discount_factor)\n",
    "            if loss is not None:\n",
    "                losses.append(loss)\n",
    "             \n",
    "    episode_rewards_history.append(episode_final_reward) # save the final reward\n",
    "    most_recent_rewards.append(episode_final_reward)\n",
    "    \n",
    "    if episode >= initial_free_exploration:\n",
    "        exploration_rate = max(min_exploration, exploration_rate * exploration_decay)\n",
    "\n",
    "    # Print per-episode result\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print(f\"[Episode {episode+1}] Reward: {episode_final_reward} | Bucket: {final_bucket} | Stars: {len(stars_collected)}\")\n",
    "\n",
    "    # print progress\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        avg_reward = sum(most_recent_rewards) / len(most_recent_rewards)\n",
    "        avg_stars = total_stars_collected / (episode + 1)\n",
    "        if losses:\n",
    "            avg_loss = sum(losses) / len(losses)\n",
    "            print(f\"Ep {episode + 1} | Avg R (last 100): {avg_reward:.2f} | Stars: {avg_stars:.2f} | Loss: {avg_loss:.4f} | ε: {exploration_rate:.2f}\")\n",
    "        else:\n",
    "            print(f\"Ep {episode + 1} | Avg R (last 100): {avg_reward:.2f} | Stars: {avg_stars:.2f} | ε: {exploration_rate:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
